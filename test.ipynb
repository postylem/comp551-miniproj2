{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comp551miniproj2 - reddit text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "5   5  The joke is on YOU!\\n\\nI've only seen it twice...            funny\n",
      "6   6  His role in MI3 is one of the best villians I'...           movies\n",
      "7   7  Akagi is still Alpha as fuck and Sugawara is s...            anime\n",
      "8   8  I think that they had each other's detonator. ...           movies\n",
      "9   9  Right! He was a disruptor tank! Pull the dps o...        Overwatch\n",
      "2968210\n",
      "   id                                           comments\n",
      "0   0  Trout and Bryant have both led the league in s...\n",
      "1   1  &gt; Just like Estonians have good reasons to ...\n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4   4         You have to take it out of the bag, Morty!\n",
      "5   5  Don't forget the obnoxious \"*memes*\" in every ...\n",
      "6   6  I say encourage local team support. Half the f...\n",
      "7   7  Favorite type of pasta? (not dish, pasta shape...\n",
      "8   8  Spinal meningitis- Ween.\\n\\nOn mobile, so no l...\n",
      "9   9  So what about Scandinavians, Caucasians, Asian...\n",
      "1257851\n"
     ]
    }
   ],
   "source": [
    "# some of the following based on tutorial from https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import sparse\n",
    "import time\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('reddit-comment-classification-comp-551/reddit_train.csv')\n",
    "df = df[pd.notnull(df['comments'])]\n",
    "print(df.head(10))\n",
    "print(df['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "X_kaggle = pd.read_csv('reddit-comment-classification-comp-551/reddit_test.csv')\n",
    "X_kaggle = X_kaggle[pd.notnull(X_kaggle['comments'])]\n",
    "print(X_kaggle.head(10))\n",
    "print(X_kaggle['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are balanced, but the text needs cleaning. Here's some cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "delimiters = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "ignored_symbols = re.compile('[^0-9a-z #+_]')\n",
    "# nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string (one comment)\n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding # our pipeline does slightly better without this.\n",
    "    text = text.lower() # lowercase text\n",
    "    text = delimiters.sub(' ', text) # replace delimiters symbols by space in text\n",
    "    text = ignored_symbols.sub('', text) # delete symbols which are in ignored_symbols from text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "df['comments'] = df['comments'].apply(clean_text)\n",
    "\n",
    "#COMPETITION SET\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to lemmatize the corpus.  This might not help in the short term, but will be useful to play with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  honestli buffalo correct answer rememb peopl s...           hockey\n",
      "1   1  ah ye way could rememb draft thought gon na gr...              nba\n",
      "2   2  http youtub 6xxbbr8isz0t40m49sif didnt find al...  leagueoflegends\n",
      "3   3  wouldnt bad sign wouldnt paid 18m euro right p...           soccer\n",
      "4   4  easi use piss dri techniqu let drop let dri ri...            funny\n",
      "5   5                              joke youiv seen twice            funny\n",
      "6   6  role mi3 one best villian ive seen movi genuin...           movies\n",
      "7   7  akagi still alpha fuck sugawara suffer definit...            anime\n",
      "8   8  think other deton wouldnt proven joker right b...           movies\n",
      "9   9  right disruptor tank pull dp frey pick get poi...        Overwatch\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "#stemmer = LancasterStemmer()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer1 = LancasterStemmer()\n",
    "\n",
    "def stem_sentence(sen):\n",
    "    \"\"\" stems every word in space separated sentence sen \"\"\" \n",
    "    token_list = word_tokenize(sen)\n",
    "    stem_sen = []\n",
    "    for w in token_list:\n",
    "#         w= stemmer1.stem(w)\n",
    "        stem_sen.append(stemmer.stem(w))\n",
    "    return \" \".join(stem_sen)\n",
    "\n",
    "# Download wordnet (could take a little while!), to do lemmatization\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# def lemm_sentence(sen):\n",
    "#     \"\"\" lemmatizes every word in space separated sentence sen \"\"\" \n",
    "#     token_list = word_tokenize(sen)\n",
    "#     lemma_sen = []\n",
    "#     for w in token_list:\n",
    "#         lemma_sen.append(lemmatizer.lemmatize(w))\n",
    "#     return \" \".join(lemma_sen)\n",
    "\n",
    "\n",
    "# choose lemm_sentence)() for lemmatization, or stem_sentence() for stemming.\n",
    "df['comments'] = df['comments'].apply(lambda x: stem_sentence(x))\n",
    "\n",
    "# print_plot(1234)\n",
    "print(df.head(10))\n",
    "\n",
    "#COMPETITION SET\n",
    "# choose lemm_sentence)() for lemmatization, or stem_sentence() for stemming.\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(lambda x: stem_sentence(x))\n",
    "X_kaggle = pd.Series(X_kaggle['comments'], index=X_kaggle.index)\n",
    "# print_plot(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set the train test split here, but we could also do some more processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_array = np.unique(df.subreddits.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to get **a list of words in the entire corpus of comments** (that is, `tokens`), and also **a list unique words** (that is `types`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# tokens_list = df['comments'].apply(lambda x: word_tokenize(x)).values\n",
    "\n",
    "# tokens = np.array(list(itertools.chain.from_iterable(tokens_list)))\n",
    "\n",
    "# types, type_counts = np.unique(tokens, return_counts=True)\n",
    "\n",
    "# print(\"number of tokens\",len(tokens))\n",
    "# print(\"number of types\",len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words are more common than others.  We might want to do something with this. But for now it's just useful to have the information.  I'll make an uncommon-word list. We can remove them just like we did the stopwords. *We'll do this later, with scikitlearn's CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_words = types[type_counts > 20]\n",
    "# len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsize_vocab(text):\n",
    "#     text = ' '.join(word for word in text.split() if word in common_words) # keep only common words\n",
    "#     return text\n",
    "    \n",
    "# df['comments'] = df['comments'].apply(downsize_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list = df['comments'].apply(lambda x: word_tokenize(x)).values\n",
    "\n",
    "# tokens = np.array(list(itertools.chain.from_iterable(tokens_list)))\n",
    "\n",
    "# types, type_counts = np.unique(tokens, return_counts=True)\n",
    "\n",
    "# print(\"number of tokens\",len(tokens))\n",
    "# print(\"number of types\",len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is cleand, set the train test split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tfidf = TfidfTransformer()\n",
    "#X_tfidf = tfidf.fit_transform(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df.subreddits, test_size=0.3, random_state = 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running sklearn classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.508429</td>\n",
       "      <td>1.864013</td>\n",
       "      <td>0.644312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.513905</td>\n",
       "      <td>1.853042</td>\n",
       "      <td>0.594410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.517381</td>\n",
       "      <td>1.732870</td>\n",
       "      <td>0.622097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.510905</td>\n",
       "      <td>1.739342</td>\n",
       "      <td>0.593918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.518095</td>\n",
       "      <td>1.931269</td>\n",
       "      <td>0.666255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.560667</td>\n",
       "      <td>1.854003</td>\n",
       "      <td>0.593413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.563619</td>\n",
       "      <td>1.889357</td>\n",
       "      <td>0.600399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>1.837084</td>\n",
       "      <td>0.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.558238</td>\n",
       "      <td>2.013577</td>\n",
       "      <td>0.659453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.560143</td>\n",
       "      <td>2.072418</td>\n",
       "      <td>0.668212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.567667</td>\n",
       "      <td>1.779242</td>\n",
       "      <td>0.645733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.571095</td>\n",
       "      <td>1.952741</td>\n",
       "      <td>0.630311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.568810</td>\n",
       "      <td>1.943799</td>\n",
       "      <td>0.726058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.563667</td>\n",
       "      <td>1.968696</td>\n",
       "      <td>0.594411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.564619</td>\n",
       "      <td>2.019499</td>\n",
       "      <td>0.795872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3\n",
       "0    BernoulliNB  0.508429  1.864013  0.644312\n",
       "1    BernoulliNB  0.513905  1.853042  0.594410\n",
       "2    BernoulliNB  0.517381  1.732870  0.622097\n",
       "3    BernoulliNB  0.510905  1.739342  0.593918\n",
       "4    BernoulliNB  0.518095  1.931269  0.666255\n",
       "0  MultinomialNB  0.560667  1.854003  0.593413\n",
       "1  MultinomialNB  0.563619  1.889357  0.600399\n",
       "2  MultinomialNB  0.563000  1.837084  0.621339\n",
       "3  MultinomialNB  0.558238  2.013577  0.659453\n",
       "4  MultinomialNB  0.560143  2.072418  0.668212\n",
       "0   ComplementNB  0.567667  1.779242  0.645733\n",
       "1   ComplementNB  0.571095  1.952741  0.630311\n",
       "2   ComplementNB  0.568810  1.943799  0.726058\n",
       "3   ComplementNB  0.563667  1.968696  0.594411\n",
       "4   ComplementNB  0.564619  2.019499  0.795872"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_nb = Pipeline([\n",
    "                         ('ct_vect', CountVectorizer(binary=True)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', BernoulliNB(alpha=0.1)),\n",
    "                        ])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multinomial_nb = Pipeline([\n",
    "                         ('ct_vect', CountVectorizer(binary=False)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB(alpha=0.2)),\n",
    "                        ])\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "complement_nb = Pipeline([\n",
    "                         ('ct_vect', CountVectorizer(binary=False)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', ComplementNB(alpha=2)),\n",
    "                        ])\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', SGDClassifier(loss='modified_huber', penalty='l2', \n",
    "                                       alpha=0.00005, random_state=None,\n",
    "                                       max_iter=10, tol=None)),\n",
    "                ])\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(ngram_range=(1,1))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', KNeighborsClassifier(n_neighbors=150, weights='distance')),\n",
    "                ])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()), # works well with more features (all 75440 ...)\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', LogisticRegression(penalty='l2',multi_class='ovr',solver='newton-cg')),\n",
    "                ])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randfrst = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(max_features=5000)),\n",
    "                 ('feature_selection', SelectFromModel(LinearSVC(C=0.1, penalty=\"l1\", dual=False))), # optional\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', RandomForestClassifier(n_estimators=10)), # more estimators might be good, but slow\n",
    "                ])\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(max_features=5000)),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MLPClassifier(alpha=1, max_iter=100)),\n",
    "                ])\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "passaggr = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', PassiveAggressiveClassifier(fit_intercept=True,C=0.1)),\n",
    "                ])\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "lda =  Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('todense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "                 ('clf', LinearDiscriminantAnalysis()),\n",
    "                ])\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda =  Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('todense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "                 ('clf', QuadraticDiscriminantAnalysis()),\n",
    "                ])\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "svc =  Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', SVC(gamma='scale', decision_function_shape='ovo')),\n",
    "                ])\n",
    "\n",
    "lsvc =  Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', LinearSVC(C=0.25)),\n",
    "                ])\n",
    "\n",
    "# now for comparing and majority voting:\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf1 = LogisticRegression(penalty='l2',multi_class='ovr',solver='saga')#l2, ovr, newton-cg try lbfgs\n",
    "clf2 = MultinomialNB(alpha=0.2) #0.25\n",
    "clf3 = ComplementNB(alpha=0.8) #0.01\n",
    "clf4 = PassiveAggressiveClassifier(fit_intercept=True,C=0.1) # won't work with voting='soft'!\n",
    "clf5 = SGDClassifier(loss='modified_huber', penalty='l2',alpha=0.00005, random_state=27,max_iter=10, tol=None)\n",
    "clf6 = KNeighborsClassifier(n_neighbors=150, weights = 'distance')\n",
    "clf7 = BernoulliNB(alpha=0.1,binarize=0.0)\n",
    "clf8 = LinearSVC(C=0.25)\n",
    "\n",
    "voting_clf = Pipeline([\n",
    "    ('ct_vect', CountVectorizer(binary=False,ngram_range=(1,1))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf' , VotingClassifier(estimators=\n",
    "                              [('logreg', clf1),('mnb', clf2), ('cnb', clf3)],# ('lsvc', clf9)],\n",
    "                                n_jobs=-1)) #voting= 'hard',\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "parameters = {\n",
    "        #passagr\n",
    "#         'clf__C':[0.5,0.1,0.01]\n",
    "        #voting\n",
    "         #'clf__voting': ['hard','None'],#,'soft'\n",
    "#         'clf__estimators':[\n",
    "#             [('logreg', clf1), ('mnb', clf2), ('cnb', clf3)], # best acc after training on whole train set\n",
    "#             [('sgd', clf5), ('mnb', clf2), ('cnb', clf3)],    # best acc in 3-fold cv, but not best after training on whole train set\n",
    "#             [('logreg', clf1), ('cnb', clf3),('sgd', clf5)],\n",
    "#             [('passaggr', clf4), ('cnb', clf3),('sgd', clf5)],\n",
    "#             [('passaggr', clf4), ('mnb', clf2), ('cnb', clf3)],\n",
    "#         ]\n",
    "        #logreg\n",
    "#         'ct_vect__ngram_range': [(1,1)],\n",
    "#         'ct_vect__max_features': [1000,5000,74000],\n",
    "#         'clf__multi_class':['ovr','multinomial'],\n",
    "#         'clf__solver':['sag','saga'],\n",
    "        \n",
    "#       #knn\n",
    "#         'ct_vect__ngram_range': [(1,1)],\n",
    "#         'ct_vect__max_features': [5000,74000],\n",
    "#         'clf__n_neighbors':[3,30],\n",
    "        \n",
    "        #bernoulli_nb\n",
    "#         'ct_vect__ngram_range': [(1,1)],\n",
    "#         'ct_vect__max_features': [10000],\n",
    "#         'clf__alpha': [1,1e-2,1e-3],\n",
    "        \n",
    "        #sgd\n",
    "#        'ct_vect__ngram_range': [(1,1),(1,2)],\n",
    "#         'ct_vect__max_features': [5000,74400],\n",
    "#         'clf__alpha': [1e-3, 1e-5],\n",
    "#         'clf__loss': ['modified_huber', 'hinge'],\n",
    "    }\n",
    "\n",
    "#               (alpha=0.0001, average=False, class_weight=None,\n",
    "#               early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "#               l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
    "#               max_iter=5, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
    "#               power_t=0.5, random_state=27, shuffle=True, tol=None,\n",
    "#               validation_fraction=0.1, verbose=0, warm_start=False\n",
    "\n",
    "def paramsearch(modelpipeline,parameters):\n",
    "    '''\n",
    "    modelpipeline: sklearn.pipeline.Pipeline object \n",
    "    does gridsearch on the given pipeline on test split and prints classification report\n",
    "    '''\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # run gridsearch for parameters with 3-fold cross validation\n",
    "    gridsearch = GridSearchCV(modelpipeline, parameters, cv=3, iid=False, n_jobs=-1)\n",
    "    gridsearch = gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "    #print(gridsearch.best_params_)\n",
    "    #print(gridsearch.best_score_)\n",
    "    print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=values_array))\n",
    "    confusion_matrix(y_test, y_pred)\n",
    "    cm=confusion_matrix(y_test,y_pred)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.matshow(cm, fignum=1)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    return gridsearch\n",
    "\n",
    "def runmodel(modelpipeline, model_name, df):\n",
    "    '''\n",
    "    modelpipeline: sklearn.pipeline.Pipeline object \n",
    "    runs the given pipeline on test split and prints classification report\n",
    "    '''\n",
    "    model = []\n",
    "    accuracy = []\n",
    "    fit_time = []\n",
    "    predict_time = []\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        model.append(model_name)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = i)\n",
    "        start_time = time.time()\n",
    "        modelpipeline.fit(X_train, y_train)\n",
    "        fit_time.append((time.time() - start_time))\n",
    "        #print(\"Fit took %.3f seconds\" % ((time.time() - start_time)))\n",
    "        start_time = time.time()\n",
    "        y_pred = modelpipeline.predict(X_test)\n",
    "        predict_time.append((time.time() - start_time))\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        #print(\"Predict took %.3f seconds\" % ((time.time() - start_time)))\n",
    "        # print(modelpipeline)\n",
    "        #print('accuracy %.3f' % accuracy_score(y_test, y_pred))\n",
    "        #print(classification_report(y_test, y_pred,target_names=values_array))\n",
    "        #confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return model, accuracy, fit_time, predict_time\n",
    "\n",
    "# start_time = time.time()\n",
    "# gridsearch = paramsearch(voting_clf,parameters)\n",
    "# print(\"took %.3f seconds\" % ((time.time() - start_time)))\n",
    "\n",
    "\n",
    "model, accuracy, fit_time, predict_time = runmodel(bernoulli_nb, \"BernoulliNB\", df)\n",
    "df1 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "model, accuracy, fit_time, predict_time = runmodel(multinomial_nb, \"MultinomialNB\",df)\n",
    "df2 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "df1 = df1.append(df2)\n",
    "model, accuracy, fit_time, predict_time = runmodel(complement_nb, \"ComplementNB\",df)\n",
    "df2 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "df1 = df1.append(df2)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store accuracy, fit_time, predict_time of SGDClassifier\n",
    "model, accuracy, fit_time, predict_time = runmodel(sgd, \"SGDClassifier\",df)\n",
    "df2 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "df1 = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store accuracy, fit_time, predict_time of KNeighborsClassifier\n",
    "model, accuracy, fit_time, predict_time = runmodel(knn, \"KNeighborsClassifier\",df)\n",
    "df2 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "df1 = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store accuracy, fit_time, predict_time of LogisticRegression\n",
    "model, accuracy, fit_time, predict_time = runmodel(logreg, \"LogisticRegression\",df)\n",
    "df2 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "df1 = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store accuracy, fit_time, predict_time of LinearSVC\n",
    "model, accuracy, fit_time, predict_time = runmodel(lsvc, \"LinearSVC\",df)\n",
    "df2 = pd.DataFrame(list(zip(model, accuracy, fit_time, predict_time)))\n",
    "df1 = df1.append(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.508429</td>\n",
       "      <td>1.864013</td>\n",
       "      <td>0.644312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.513905</td>\n",
       "      <td>1.853042</td>\n",
       "      <td>0.594410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.517381</td>\n",
       "      <td>1.732870</td>\n",
       "      <td>0.622097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.510905</td>\n",
       "      <td>1.739342</td>\n",
       "      <td>0.593918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.518095</td>\n",
       "      <td>1.931269</td>\n",
       "      <td>0.666255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.560667</td>\n",
       "      <td>1.854003</td>\n",
       "      <td>0.593413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.563619</td>\n",
       "      <td>1.889357</td>\n",
       "      <td>0.600399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>1.837084</td>\n",
       "      <td>0.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.558238</td>\n",
       "      <td>2.013577</td>\n",
       "      <td>0.659453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.560143</td>\n",
       "      <td>2.072418</td>\n",
       "      <td>0.668212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.567667</td>\n",
       "      <td>1.779242</td>\n",
       "      <td>0.645733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.571095</td>\n",
       "      <td>1.952741</td>\n",
       "      <td>0.630311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.568810</td>\n",
       "      <td>1.943799</td>\n",
       "      <td>0.726058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.563667</td>\n",
       "      <td>1.968696</td>\n",
       "      <td>0.594411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.564619</td>\n",
       "      <td>2.019499</td>\n",
       "      <td>0.795872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.480286</td>\n",
       "      <td>1.639612</td>\n",
       "      <td>41.361456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>1.564325</td>\n",
       "      <td>40.330163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.486143</td>\n",
       "      <td>1.635587</td>\n",
       "      <td>36.667245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.483190</td>\n",
       "      <td>1.610654</td>\n",
       "      <td>39.951704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.487762</td>\n",
       "      <td>1.527873</td>\n",
       "      <td>40.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.550381</td>\n",
       "      <td>13.193981</td>\n",
       "      <td>1.408679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.556905</td>\n",
       "      <td>13.049543</td>\n",
       "      <td>1.377385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.553667</td>\n",
       "      <td>12.945541</td>\n",
       "      <td>1.375323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.548524</td>\n",
       "      <td>13.065738</td>\n",
       "      <td>1.400823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.550238</td>\n",
       "      <td>13.347994</td>\n",
       "      <td>1.419204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.534524</td>\n",
       "      <td>23.579753</td>\n",
       "      <td>0.562493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.539333</td>\n",
       "      <td>23.082140</td>\n",
       "      <td>0.574466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.534143</td>\n",
       "      <td>25.756263</td>\n",
       "      <td>0.607375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.532571</td>\n",
       "      <td>24.497077</td>\n",
       "      <td>0.604382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>25.642534</td>\n",
       "      <td>0.688162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.556238</td>\n",
       "      <td>6.007488</td>\n",
       "      <td>0.627322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.559571</td>\n",
       "      <td>5.565986</td>\n",
       "      <td>0.597403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.557619</td>\n",
       "      <td>5.398160</td>\n",
       "      <td>0.631309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>5.427018</td>\n",
       "      <td>0.605380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.556476</td>\n",
       "      <td>6.029585</td>\n",
       "      <td>0.572484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0         1          2          3\n",
       "0            BernoulliNB  0.508429   1.864013   0.644312\n",
       "1            BernoulliNB  0.513905   1.853042   0.594410\n",
       "2            BernoulliNB  0.517381   1.732870   0.622097\n",
       "3            BernoulliNB  0.510905   1.739342   0.593918\n",
       "4            BernoulliNB  0.518095   1.931269   0.666255\n",
       "5          MultinomialNB  0.560667   1.854003   0.593413\n",
       "6          MultinomialNB  0.563619   1.889357   0.600399\n",
       "7          MultinomialNB  0.563000   1.837084   0.621339\n",
       "8          MultinomialNB  0.558238   2.013577   0.659453\n",
       "9          MultinomialNB  0.560143   2.072418   0.668212\n",
       "10          ComplementNB  0.567667   1.779242   0.645733\n",
       "11          ComplementNB  0.571095   1.952741   0.630311\n",
       "12          ComplementNB  0.568810   1.943799   0.726058\n",
       "13          ComplementNB  0.563667   1.968696   0.594411\n",
       "14          ComplementNB  0.564619   2.019499   0.795872\n",
       "15  KNeighborsClassifier  0.480286   1.639612  41.361456\n",
       "16  KNeighborsClassifier  0.492000   1.564325  40.330163\n",
       "17  KNeighborsClassifier  0.486143   1.635587  36.667245\n",
       "18  KNeighborsClassifier  0.483190   1.610654  39.951704\n",
       "19  KNeighborsClassifier  0.487762   1.527873  40.076400\n",
       "20         SGDClassifier  0.550381  13.193981   1.408679\n",
       "21         SGDClassifier  0.556905  13.049543   1.377385\n",
       "22         SGDClassifier  0.553667  12.945541   1.375323\n",
       "23         SGDClassifier  0.548524  13.065738   1.400823\n",
       "24         SGDClassifier  0.550238  13.347994   1.419204\n",
       "25    LogisticRegression  0.534524  23.579753   0.562493\n",
       "26    LogisticRegression  0.539333  23.082140   0.574466\n",
       "27    LogisticRegression  0.534143  25.756263   0.607375\n",
       "28    LogisticRegression  0.532571  24.497077   0.604382\n",
       "29    LogisticRegression  0.537000  25.642534   0.688162\n",
       "30             LinearSVC  0.556238   6.007488   0.627322\n",
       "31             LinearSVC  0.559571   5.565986   0.597403\n",
       "32             LinearSVC  0.557619   5.398160   0.631309\n",
       "33             LinearSVC  0.549619   5.427018   0.605380\n",
       "34             LinearSVC  0.556476   6.029585   0.572484"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('accuracies_runtimes.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit_time</th>\n",
       "      <th>Predict_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.508429</td>\n",
       "      <td>1.864013</td>\n",
       "      <td>0.644312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.513905</td>\n",
       "      <td>1.853042</td>\n",
       "      <td>0.594410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.517381</td>\n",
       "      <td>1.732870</td>\n",
       "      <td>0.622097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.510905</td>\n",
       "      <td>1.739342</td>\n",
       "      <td>0.593918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.518095</td>\n",
       "      <td>1.931269</td>\n",
       "      <td>0.666255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.560667</td>\n",
       "      <td>1.854003</td>\n",
       "      <td>0.593413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.563619</td>\n",
       "      <td>1.889357</td>\n",
       "      <td>0.600399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>1.837084</td>\n",
       "      <td>0.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.558238</td>\n",
       "      <td>2.013577</td>\n",
       "      <td>0.659453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.560143</td>\n",
       "      <td>2.072418</td>\n",
       "      <td>0.668212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.567667</td>\n",
       "      <td>1.779242</td>\n",
       "      <td>0.645733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.571095</td>\n",
       "      <td>1.952741</td>\n",
       "      <td>0.630311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.568810</td>\n",
       "      <td>1.943799</td>\n",
       "      <td>0.726058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.563667</td>\n",
       "      <td>1.968696</td>\n",
       "      <td>0.594411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>ComplementNB</td>\n",
       "      <td>0.564619</td>\n",
       "      <td>2.019499</td>\n",
       "      <td>0.795872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.480286</td>\n",
       "      <td>1.639612</td>\n",
       "      <td>41.361456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>1.564325</td>\n",
       "      <td>40.330163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.486143</td>\n",
       "      <td>1.635587</td>\n",
       "      <td>36.667245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.483190</td>\n",
       "      <td>1.610654</td>\n",
       "      <td>39.951704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.487762</td>\n",
       "      <td>1.527873</td>\n",
       "      <td>40.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.550381</td>\n",
       "      <td>13.193981</td>\n",
       "      <td>1.408679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.556905</td>\n",
       "      <td>13.049543</td>\n",
       "      <td>1.377385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.553667</td>\n",
       "      <td>12.945541</td>\n",
       "      <td>1.375323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.548524</td>\n",
       "      <td>13.065738</td>\n",
       "      <td>1.400823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.550238</td>\n",
       "      <td>13.347994</td>\n",
       "      <td>1.419204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.534524</td>\n",
       "      <td>23.579753</td>\n",
       "      <td>0.562493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.539333</td>\n",
       "      <td>23.082140</td>\n",
       "      <td>0.574466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.534143</td>\n",
       "      <td>25.756263</td>\n",
       "      <td>0.607375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.532571</td>\n",
       "      <td>24.497077</td>\n",
       "      <td>0.604382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>25.642534</td>\n",
       "      <td>0.688162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.556238</td>\n",
       "      <td>6.007488</td>\n",
       "      <td>0.627322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.559571</td>\n",
       "      <td>5.565986</td>\n",
       "      <td>0.597403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.557619</td>\n",
       "      <td>5.398160</td>\n",
       "      <td>0.631309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.549619</td>\n",
       "      <td>5.427018</td>\n",
       "      <td>0.605380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.556476</td>\n",
       "      <td>6.029585</td>\n",
       "      <td>0.572484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy   Fit_time  Predict_time\n",
       "0            BernoulliNB  0.508429   1.864013      0.644312\n",
       "1            BernoulliNB  0.513905   1.853042      0.594410\n",
       "2            BernoulliNB  0.517381   1.732870      0.622097\n",
       "3            BernoulliNB  0.510905   1.739342      0.593918\n",
       "4            BernoulliNB  0.518095   1.931269      0.666255\n",
       "5          MultinomialNB  0.560667   1.854003      0.593413\n",
       "6          MultinomialNB  0.563619   1.889357      0.600399\n",
       "7          MultinomialNB  0.563000   1.837084      0.621339\n",
       "8          MultinomialNB  0.558238   2.013577      0.659453\n",
       "9          MultinomialNB  0.560143   2.072418      0.668212\n",
       "10          ComplementNB  0.567667   1.779242      0.645733\n",
       "11          ComplementNB  0.571095   1.952741      0.630311\n",
       "12          ComplementNB  0.568810   1.943799      0.726058\n",
       "13          ComplementNB  0.563667   1.968696      0.594411\n",
       "14          ComplementNB  0.564619   2.019499      0.795872\n",
       "15  KNeighborsClassifier  0.480286   1.639612     41.361456\n",
       "16  KNeighborsClassifier  0.492000   1.564325     40.330163\n",
       "17  KNeighborsClassifier  0.486143   1.635587     36.667245\n",
       "18  KNeighborsClassifier  0.483190   1.610654     39.951704\n",
       "19  KNeighborsClassifier  0.487762   1.527873     40.076400\n",
       "20         SGDClassifier  0.550381  13.193981      1.408679\n",
       "21         SGDClassifier  0.556905  13.049543      1.377385\n",
       "22         SGDClassifier  0.553667  12.945541      1.375323\n",
       "23         SGDClassifier  0.548524  13.065738      1.400823\n",
       "24         SGDClassifier  0.550238  13.347994      1.419204\n",
       "25    LogisticRegression  0.534524  23.579753      0.562493\n",
       "26    LogisticRegression  0.539333  23.082140      0.574466\n",
       "27    LogisticRegression  0.534143  25.756263      0.607375\n",
       "28    LogisticRegression  0.532571  24.497077      0.604382\n",
       "29    LogisticRegression  0.537000  25.642534      0.688162\n",
       "30             LinearSVC  0.556238   6.007488      0.627322\n",
       "31             LinearSVC  0.559571   5.565986      0.597403\n",
       "32             LinearSVC  0.557619   5.398160      0.631309\n",
       "33             LinearSVC  0.549619   5.427018      0.605380\n",
       "34             LinearSVC  0.556476   6.029585      0.572484"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rename(columns={\"0\": \"Model\", \"1\": \"Accuracy\", \"2\": \"Fit_time\", \"3\": \"Predict_time\"}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"accuracies_runtimes.csv\", index=False, sep = ',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.566\n",
      "took 1.944 seconds for ComplementNB(alpha=1, class_prior=None, fit_prior=True, norm=False)\n",
      "accuracy 0.558\n",
      "took 1.988 seconds for MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "accuracy 0.552\n",
      "took 8.945 seconds for SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
      "              max_iter=5, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=27, shuffle=True, tol=None,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "accuracy 0.540\n",
      "took 12.768 seconds for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "accuracy 0.539\n",
      "took 5.923 seconds for PassiveAggressiveClassifier(C=0.1, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=None, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "accuracy 0.517\n",
      "took 1.987 seconds for BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "accuracy 0.483\n",
      "took 32.988 seconds for KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=300, p=2,\n",
      "                     weights='distance')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = [\n",
    "    complement_nb,\n",
    "    multinomial_nb,\n",
    "    sgd,\n",
    "    logreg,\n",
    "    passaggr,\n",
    "    bernoulli_nb,\n",
    "    knn\n",
    "    # randfrst, #only gets 37\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    runmodel(model)\n",
    "    print(\"took %.3f seconds for %s\" % ((time.time() - start_time), model['clf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# adaboost = Pipeline([\n",
    "#                  ('ct_vect', CountVectorizer()),\n",
    "#                  ('tfidf', TfidfTransformer()),\n",
    "#                  ('clf', AdaBoostClassifier(\n",
    "#                     ComplementNB(alpha=1),\n",
    "#                     n_estimators=200,\n",
    "#                     learning_rate=1))\n",
    "#                 ]) ## currently getting accuracy 0.44776\n",
    "\n",
    "# adaparameters = {\n",
    "# #         'ct_vect__ngram_range': [(1,1),(1,2)],\n",
    "# #         'ct_vect__max_features': [5000,7000],\n",
    "# #         'clf__base_estimator__alpha': [1, 0.1, 1e-3],\n",
    "# #         'clf__learning_rate': [0.001,0.1,1],\n",
    "#     }\n",
    "# adagrid = paramsearch(adaboost,adaparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['baseball', 'europe', 'anime', ..., 'GlobalOffensive',\n",
       "       'gameofthrones', 'wow'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKE PREDICTIONS FOR THE HELDOUT COMPETITION SET\n",
    "y_kaggle = gridsearch.predict(X_kaggle)\n",
    "y_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id         Category\n",
       "0          0         baseball\n",
       "1          1           europe\n",
       "2          2            anime\n",
       "3          3        worldnews\n",
       "4          4            trees\n",
       "...      ...              ...\n",
       "29995  29995           movies\n",
       "29996  29996            Music\n",
       "29997  29997  GlobalOffensive\n",
       "29998  29998    gameofthrones\n",
       "29999  29999              wow\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_kaggle = pd.Series(y_kaggle, index=X_kaggle.index)\n",
    "y_kaggle = pd.DataFrame(y_kaggle)\n",
    "y_kaggle['Id_'] = y_kaggle.index\n",
    "y_kaggle.insert(loc = 0, column = \"Id\", value = y_kaggle['Id_'])\n",
    "y_kaggle = y_kaggle.drop(columns=['Id_'])\n",
    "y_kaggle = y_kaggle.rename(columns={0: \"Category\"})\n",
    "y_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kaggle.to_csv(\"predictions577_582.csv\", index=False, sep = ',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(observations, y, num_features,smoothing):\n",
    "\n",
    "    #Initialize marginal probability for each class\n",
    "    count_class = np.array(20*[[0]])\n",
    "    marg_prob = np.array(20*[[0]]) #Laplace smoothing, starting counts with 1\n",
    "\n",
    "    #Initialize matrix of probabilities of observed features given k\n",
    "    cond_prob_matrix = np.ones((20,num_features)) * smoothing\n",
    "\n",
    "    \n",
    "    #compute marginal probability of each class\n",
    "    total_comments = y.shape[0]\n",
    "    for j in range(total_comments):\n",
    "        count_class[y[j]] += 1\n",
    "    \n",
    "    #Marginal probability for each class\n",
    "    marg_prob = np.true_divide(count_class, total_comments)\n",
    "    \n",
    "    observ = observations.nonzero()\n",
    "    j = 0 #counter of comments\n",
    "    prev_comment_no = observ[0][0] #counter to see if next comment\n",
    "    for i in range(observations.shape[0]):\n",
    "        \n",
    "        feature_no = observ[1][i]\n",
    "        comment_no = observ[0][i]\n",
    "        \n",
    "        if prev_comment_no != comment_no:\n",
    "            j += comment_no - prev_comment_no\n",
    "            prev_comment_no = comment_no\n",
    "            \n",
    "        comment_class = y[j]\n",
    "        cond_prob_matrix[comment_class][feature_no] += 1\n",
    "\n",
    "    #divide each row of cond_prob_matrix by the count of comments per class\n",
    "    for i in range(20):\n",
    "        cond_prob_matrix[i] = np.true_divide(cond_prob_matrix[i], count_class[i])\n",
    "\n",
    "    cond_prob_matrix = cond_prob_matrix.transpose()\n",
    "    marg_prob = np.log(marg_prob)\n",
    "\n",
    "    #marg_prob is a vector of 20, cond_prob_matrix a matrix #features rows by 20\n",
    "    return marg_prob, cond_prob_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "        \"anime\": 1,\n",
    "        \"AskReddit\": 2,\n",
    "        \"baseball\": 3,\n",
    "        \"canada\": 4, \n",
    "        \"conspiracy\": 5, \n",
    "        \"europe\": 6, \n",
    "        \"funny\": 7, \n",
    "        \"gameofthrones\": 8, \n",
    "        \"GlobalOffensive\": 9,\n",
    "        \"hockey\" :10, \n",
    "        \"leagueoflegends\": 11, \n",
    "        \"movies\": 12, \n",
    "        \"Music\": 13, \n",
    "        \"nba\":14, \n",
    "        \"nfl\":15, \n",
    "        \"Overwatch\":16, \n",
    "        \"soccer\":17, \n",
    "        \"trees\":18, \n",
    "        \"worldnews\":19, \n",
    "        \"wow\":0\n",
    "    }\n",
    "\n",
    "y_traindf = pd.DataFrame(y_train)\n",
    "y_traindf['subreddits']= y_traindf['subreddits'].map(classes)\n",
    "y_train_array = np.array(y_traindf['subreddits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000,binary=True)\n",
    "X_train_tf = cv.fit_transform(X_train)\n",
    "X_test_tf = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.46822500228881836 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# from naive_bayes import fit_naive_bayes\n",
    "prior, conditional = fit_naive_bayes(X_train_tf, y_train_array, X_train_tf.shape[1],0.01)\n",
    "\n",
    "print(\"Took %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.10004100e-06, 4.01606426e-06, 4.08496732e-06, ...,\n",
       "        4.00160064e-06, 4.05022276e-06, 4.14937759e-06],\n",
       "       [4.10004100e-06, 4.01606426e-06, 4.08496732e-06, ...,\n",
       "        4.04161665e-04, 4.09072499e-04, 4.14937759e-06],\n",
       "       [4.10004100e-06, 4.01606426e-06, 4.08496732e-06, ...,\n",
       "        4.00160064e-06, 4.05022276e-06, 4.19087137e-04],\n",
       "       ...,\n",
       "       [8.24108241e-04, 4.01606426e-06, 4.08496732e-06, ...,\n",
       "        4.00160064e-06, 4.09072499e-04, 4.14937759e-06],\n",
       "       [4.10004100e-06, 4.01606426e-06, 4.08496732e-06, ...,\n",
       "        4.00160064e-06, 4.05022276e-06, 4.14937759e-06],\n",
       "       [4.10004100e-06, 4.01606426e-06, 4.08496732e-06, ...,\n",
       "        4.00160064e-06, 4.05022276e-06, 4.14937759e-06]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "def predict_naive_bayes(id_list, observations, marg_prob, cond_prob_matrix):\n",
    "\n",
    "    #log of inverse conditional probability matrix\n",
    "    inv_cond_prob_matrix = np.ones((cond_prob_matrix.shape[0], cond_prob_matrix.shape[1]), dtype=int)\n",
    "    inv_cond_prob_matrix = inv_cond_prob_matrix - cond_prob_matrix\n",
    "    inv_cond_prob_matrix = sparse.csr_matrix(np.log(inv_cond_prob_matrix))\n",
    "\n",
    "    #log of conditional probability matrix\n",
    "    cond_prob_matrix = sparse.csr_matrix(np.log(cond_prob_matrix))\n",
    "    \n",
    "    # 0s become 1s, 1s become 0s\n",
    "    sparse_ones = sparse.csr_matrix(np.ones((observations.shape[0], observations.shape[1])), dtype=int)\n",
    "    complement_obs = sparse_ones - observations\n",
    "    \n",
    "    prob_per_class = observations.dot(cond_prob_matrix) + complement_obs.dot(inv_cond_prob_matrix)\n",
    "    \n",
    "    y = []\n",
    "    for i in range(observations.shape[0]):\n",
    "        prob_per_class[i] += marg_prob.transpose()\n",
    "        y.append(np.argmax(prob_per_class[i]))\n",
    "        \n",
    "    id_list = np.array(id_list).transpose()\n",
    "\n",
    "    matrix = np.stack((id_list, y)).transpose()\n",
    "    df_pred = pd.DataFrame(matrix)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_naive_bayes(ID_list, X_test_tf, prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-d3047e638106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomment_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconditional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                     \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomment_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mconditional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mcomment_no\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr_comment_no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mclear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_features = X_test_tf.shape[1]\n",
    "\n",
    "prediction = np.zeros((conditional.shape[0],20))\n",
    "obs = X_test_tf.nonzero()\n",
    "comment_no = obs[0][0]\n",
    "feat_in_comment = []\n",
    "i = comment_no\n",
    "clear = 0\n",
    "while i < (X_test_tf.shape[0]):\n",
    "    curr_comment_no = obs[0][i]\n",
    "    if clear == 1:\n",
    "        feat_in_comment = []\n",
    "        clear = 0\n",
    "    if comment_no == curr_comment_no: #get all the words present in a comment\n",
    "        feat_in_comment.append(obs[1][i])\n",
    "        i += 1\n",
    "    else:\n",
    "        for c in range(20): #for each class\n",
    "            for j in range(num_features): #for each feature\n",
    "                if j in feat_in_comment: #if feature j appears in comment\n",
    "                    prediction[comment_no][c] += np.log(conditional[j][c])\n",
    "                else:\n",
    "                    prediction[comment_no][c] += np.log(1-conditional[j][c])\n",
    "        comment_no = curr_comment_no\n",
    "        clear = 1\n",
    "        if comment_no % 20 == 0:\n",
    "            print(comment_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12045</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5239</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>36260</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>33645</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>51680</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20995</td>\n",
       "      <td>55928</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20996</td>\n",
       "      <td>17000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20997</td>\n",
       "      <td>68828</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20998</td>\n",
       "      <td>42940</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20999</td>\n",
       "      <td>62239</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0   1\n",
       "0      12045  13\n",
       "1       5239   2\n",
       "2      36260   8\n",
       "3      33645  16\n",
       "4      51680  13\n",
       "...      ...  ..\n",
       "20995  55928  15\n",
       "20996  17000   0\n",
       "20997  68828  19\n",
       "20998  42940  19\n",
       "20999  62239  16\n",
       "\n",
       "[21000 rows x 2 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = \n",
    "prediction = np.zeros((cond_prob_matrix.shape[0],20))\n",
    "obs = observations.nonzero()\n",
    "i =  obs[0][0]\n",
    "k = i\n",
    "while i < (obs.shape[0]): #iterating through first colomn of sparse observation.nonzero() matrix\n",
    "    \n",
    "    for c in range(20):\n",
    "        \n",
    "        i =  obs[0][0]\n",
    "        for j in range(observations.shape[1]):\n",
    "            comment_no = obs[0][i]\n",
    "            if j == obs[1][i]:\n",
    "                prediction[][c] += (prob to define)\n",
    "                \n",
    "                if obs[0][i+1] == obs[0][i]: \n",
    "                    i+=1\n",
    "            else:\n",
    "                prediction[][c] += 1- (prob to define)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testdf = pd.DataFrame(y_test)\n",
    "y_testdf['subreddits']= y_testdf['subreddits'].map(classes)\n",
    "y_test_array = np.array(y_testdf['subreddits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df_pred, df_true_y):\n",
    "\n",
    "    pred = np.array(df_pred[1])\n",
    "    true_y = np.array(df_true_y['subreddits'])\n",
    "\n",
    "    count = 0\n",
    "    total = len(true_y)\n",
    "    for i in range(total):\n",
    "        if pred[i] == true_y[i]:\n",
    "            count +=1\n",
    "            \n",
    "    return float(count)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2987142857142857\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(predictions, y_testdf))\n",
    "#print(classification_report(y_testdf['subreddits'], predictions[1],target_names=values_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another implementation of Bernoulli, this one from an online tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnotherBernoulliNB(object):\n",
    "    ''' based on https://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html '''\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [np.log(len(i) / count_sample) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        smoothing = 2 * self.alpha\n",
    "        n_doc = np.array([len(i) + smoothing for i in separated])\n",
    "        self.feature_prob_ = count / n_doc[np.newaxis].T\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return [(np.log(self.feature_prob_) * x + \\\n",
    "                 np.log(1 - self.feature_prob_) * np.abs(x - 1)\n",
    "                ).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = BernoulliNB(alpha=0.1)\n",
    "nb.fit(X_train_tf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpredictions = nb.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12045</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5239</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36260</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33645</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51680</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55928</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68828</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42940</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62239</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddits\n",
       "12045   baseball\n",
       "5239   AskReddit\n",
       "36260  AskReddit\n",
       "33645  AskReddit\n",
       "51680      funny\n",
       "...          ...\n",
       "55928     hockey\n",
       "17000        wow\n",
       "68828     europe\n",
       "42940     soccer\n",
       "62239      funny\n",
       "\n",
       "[21000 rows x 1 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame(y_test)\n",
    "prediction_df['subreddits'] = pd.DataFrame(newpredictions)[0].values\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.48533333333333334\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.27      0.63      0.38      1063\n",
      "GlobalOffensive       0.38      0.19      0.25      1056\n",
      "          Music       0.14      0.13      0.14      1003\n",
      "      Overwatch       0.52      0.09      0.16      1060\n",
      "          anime       0.20      0.40      0.26      1030\n",
      "       baseball       0.26      0.13      0.18      1047\n",
      "         canada       0.26      0.37      0.30      1039\n",
      "     conspiracy       0.16      0.06      0.09      1084\n",
      "         europe       0.47      0.55      0.51      1083\n",
      "          funny       0.53      0.14      0.22      1030\n",
      "  gameofthrones       0.35      0.31      0.33      1059\n",
      "         hockey       0.43      0.21      0.28      1043\n",
      "leagueoflegends       0.31      0.29      0.30      1051\n",
      "         movies       0.24      0.62      0.34      1050\n",
      "            nba       0.51      0.19      0.28      1025\n",
      "            nfl       0.32      0.41      0.36      1055\n",
      "         soccer       0.34      0.51      0.41      1049\n",
      "          trees       0.51      0.22      0.31      1053\n",
      "      worldnews       0.46      0.12      0.19      1074\n",
      "            wow       0.16      0.25      0.20      1046\n",
      "\n",
      "       accuracy                           0.29     21000\n",
      "      macro avg       0.34      0.29      0.27     21000\n",
      "   weighted avg       0.34      0.29      0.27     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_test, prediction_df))\n",
    "print(classification_report(y_testdf['subreddits'], predictions[1],target_names=values_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
