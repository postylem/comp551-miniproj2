{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                           comments       subreddits\n",
      "0    0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1    1  Ah yes way could have been :( remember when he...              nba\n",
      "2    2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3    3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4    4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "5    5  The joke is on YOU!\\n\\nI've only seen it twice...            funny\n",
      "6    6  His role in MI3 is one of the best villians I'...           movies\n",
      "7    7  Akagi is still Alpha as fuck and Sugawara is s...            anime\n",
      "8    8  I think that they had each other's detonator. ...           movies\n",
      "9    9  Right! He was a disruptor tank! Pull the dps o...        Overwatch\n",
      "10  10  The flying the Eagles to Mordor thing is incre...           movies\n",
      "11  11  \"Oh man I can't wait to vote.\"\\n\\n*opens link*...            anime\n",
      "12  12  omg i was thinking the same.... azumi u the al...            anime\n",
      "13  13  One shot, one kill!\\nWait... that's not the ri...        Overwatch\n",
      "14  14  I'm new to this sub and I was curious. Is the ...            trees\n",
      "15  15  I pay $220/oz in Brooklyn for mid range trees....            trees\n",
      "16  16  I'm glad you're considering a rewatch. This se...            anime\n",
      "17  17  And it's been the same stories all window, sam...           soccer\n",
      "18  18  Not willing to negotiate a contract well after...           soccer\n",
      "19  19  Afraid I'll get addicted and fail school or sm...  GlobalOffensive\n",
      "2968210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conspiracy         3500\n",
       "worldnews          3500\n",
       "wow                3500\n",
       "hockey             3500\n",
       "funny              3500\n",
       "anime              3500\n",
       "GlobalOffensive    3500\n",
       "baseball           3500\n",
       "nba                3500\n",
       "AskReddit          3500\n",
       "Music              3500\n",
       "leagueoflegends    3500\n",
       "europe             3500\n",
       "canada             3500\n",
       "gameofthrones      3500\n",
       "Overwatch          3500\n",
       "movies             3500\n",
       "soccer             3500\n",
       "nfl                3500\n",
       "trees              3500\n",
       "Name: subreddits, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "df = pd.read_csv('reddit-comment-classification-comp-551/reddit_train.csv')\n",
    "df = df[pd.notnull(df['comments'])]\n",
    "print(df.head(20))\n",
    "print(df['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "\n",
    "df.subreddits.value_counts().plot(kind='bar');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/j/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def print_plot(index):\n",
    "    example = df[df.index == index][['comments', 'subreddits']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('subreddit:', example[1])\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "df['comments'] = df['comments'].apply(clean_text)\n",
    "df['subreddits'].apply(lambda x: len(x.split(' '))).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I guess all those finals and top 4 dont count anymore because of one bad tournament. Sick logic you got there\n",
      "subreddit: GlobalOffensive\n"
     ]
    }
   ],
   "source": [
    "print_plot(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.comments\n",
    "y = df.subreddits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_array = np.unique(df.subreddits.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoulli accuracy 0.47828571428571426\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.27      0.20      0.23      1023\n",
      "GlobalOffensive       0.40      0.67      0.50      1051\n",
      "          Music       0.82      0.29      0.43      1055\n",
      "      Overwatch       0.72      0.60      0.65      1002\n",
      "          anime       0.74      0.42      0.54      1075\n",
      "       baseball       0.52      0.59      0.55      1067\n",
      "         canada       0.48      0.38      0.42       995\n",
      "     conspiracy       0.50      0.29      0.37      1036\n",
      "         europe       0.57      0.35      0.44      1018\n",
      "          funny       0.13      0.48      0.21      1087\n",
      "  gameofthrones       0.84      0.61      0.71      1024\n",
      "         hockey       0.58      0.60      0.59      1043\n",
      "leagueoflegends       0.86      0.51      0.64      1084\n",
      "         movies       0.70      0.46      0.56      1095\n",
      "            nba       0.50      0.65      0.56      1055\n",
      "            nfl       0.71      0.55      0.62      1010\n",
      "         soccer       0.83      0.51      0.63      1120\n",
      "          trees       0.28      0.63      0.39      1060\n",
      "      worldnews       0.51      0.20      0.28      1065\n",
      "            wow       0.89      0.57      0.70      1035\n",
      "\n",
      "       accuracy                           0.48     21000\n",
      "      macro avg       0.59      0.48      0.50     21000\n",
      "   weighted avg       0.59      0.48      0.50     21000\n",
      "\n",
      "multinomial accuracy 0.47828571428571426\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.27      0.20      0.23      1023\n",
      "GlobalOffensive       0.40      0.67      0.50      1051\n",
      "          Music       0.82      0.29      0.43      1055\n",
      "      Overwatch       0.72      0.60      0.65      1002\n",
      "          anime       0.74      0.42      0.54      1075\n",
      "       baseball       0.52      0.59      0.55      1067\n",
      "         canada       0.48      0.38      0.42       995\n",
      "     conspiracy       0.50      0.29      0.37      1036\n",
      "         europe       0.57      0.35      0.44      1018\n",
      "          funny       0.13      0.48      0.21      1087\n",
      "  gameofthrones       0.84      0.61      0.71      1024\n",
      "         hockey       0.58      0.60      0.59      1043\n",
      "leagueoflegends       0.86      0.51      0.64      1084\n",
      "         movies       0.70      0.46      0.56      1095\n",
      "            nba       0.50      0.65      0.56      1055\n",
      "            nfl       0.71      0.55      0.62      1010\n",
      "         soccer       0.83      0.51      0.63      1120\n",
      "          trees       0.28      0.63      0.39      1060\n",
      "      worldnews       0.51      0.20      0.28      1065\n",
      "            wow       0.89      0.57      0.70      1035\n",
      "\n",
      "       accuracy                           0.48     21000\n",
      "      macro avg       0.59      0.48      0.50     21000\n",
      "   weighted avg       0.59      0.48      0.50     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "bernoulli_nb = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', BernoulliNB()),\n",
    "                        ])\n",
    "bernoulli_nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "multinomial_nb = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', BernoulliNB()),\n",
    "                        ])\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred1 = bernoulli_nb.predict(X_test)\n",
    "y_pred2 = multinomial_nb.predict(X_test)\n",
    "\n",
    "print('bernoulli accuracy %s' % accuracy_score(y_pred1, y_test))\n",
    "print(classification_report(y_test, y_pred1,target_names=values_array))\n",
    "\n",
    "print('multinomial accuracy %s' % accuracy_score(y_pred2, y_test))\n",
    "print(classification_report(y_test, y_pred2,target_names=values_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
