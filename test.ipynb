{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comp551miniproj2 - reddit text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                           comments       subreddits\n",
      "0    0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1    1  Ah yes way could have been :( remember when he...              nba\n",
      "2    2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3    3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4    4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "5    5  The joke is on YOU!\\n\\nI've only seen it twice...            funny\n",
      "6    6  His role in MI3 is one of the best villians I'...           movies\n",
      "7    7  Akagi is still Alpha as fuck and Sugawara is s...            anime\n",
      "8    8  I think that they had each other's detonator. ...           movies\n",
      "9    9  Right! He was a disruptor tank! Pull the dps o...        Overwatch\n",
      "10  10  The flying the Eagles to Mordor thing is incre...           movies\n",
      "11  11  \"Oh man I can't wait to vote.\"\\n\\n*opens link*...            anime\n",
      "12  12  omg i was thinking the same.... azumi u the al...            anime\n",
      "13  13  One shot, one kill!\\nWait... that's not the ri...        Overwatch\n",
      "14  14  I'm new to this sub and I was curious. Is the ...            trees\n",
      "15  15  I pay $220/oz in Brooklyn for mid range trees....            trees\n",
      "16  16  I'm glad you're considering a rewatch. This se...            anime\n",
      "17  17  And it's been the same stories all window, sam...           soccer\n",
      "18  18  Not willing to negotiate a contract well after...           soccer\n",
      "19  19  Afraid I'll get addicted and fail school or sm...  GlobalOffensive\n",
      "2968210\n",
      "    id                                           comments       subreddits\n",
      "0    0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1    1  Ah yes way could have been :( remember when he...              nba\n",
      "2    2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3    3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4    4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "5    5  The joke is on YOU!\\n\\nI've only seen it twice...            funny\n",
      "6    6  His role in MI3 is one of the best villians I'...           movies\n",
      "7    7  Akagi is still Alpha as fuck and Sugawara is s...            anime\n",
      "8    8  I think that they had each other's detonator. ...           movies\n",
      "9    9  Right! He was a disruptor tank! Pull the dps o...        Overwatch\n",
      "10  10  The flying the Eagles to Mordor thing is incre...           movies\n",
      "11  11  \"Oh man I can't wait to vote.\"\\n\\n*opens link*...            anime\n",
      "12  12  omg i was thinking the same.... azumi u the al...            anime\n",
      "13  13  One shot, one kill!\\nWait... that's not the ri...        Overwatch\n",
      "14  14  I'm new to this sub and I was curious. Is the ...            trees\n",
      "15  15  I pay $220/oz in Brooklyn for mid range trees....            trees\n",
      "16  16  I'm glad you're considering a rewatch. This se...            anime\n",
      "17  17  And it's been the same stories all window, sam...           soccer\n",
      "18  18  Not willing to negotiate a contract well after...           soccer\n",
      "19  19  Afraid I'll get addicted and fail school or sm...  GlobalOffensive\n",
      "1257851\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# based on tutorial from https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "df = pd.read_csv('reddit-comment-classification-comp-551/reddit_train.csv')\n",
    "df = df[pd.notnull(df['comments'])]\n",
    "print(df.head(20))\n",
    "print(df['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "X_kaggle = pd.read_csv('reddit-comment-classification-comp-551/reddit_test.csv')\n",
    "X_kaggle = X_kaggle[pd.notnull(X_kaggle['comments'])]\n",
    "print(df.head(20))\n",
    "print(X_kaggle['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "df.subreddits.value_counts().plot(kind='bar');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are balanced, but the text needs cleaning. Here's some cleaning (we should customize **TODO: we should make something to remove links... that is words starting with `http://` at least, should be ignored**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "ignored_symbols = re.compile('[^0-9a-z #+_]')\n",
    "# nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# def print_plot(index):\n",
    "#     example = df[df.index == index][['comments', 'subreddits']].values[0]\n",
    "#     if len(example) > 0:\n",
    "#         print(example[0])\n",
    "#         print('subreddit:', example[1])\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string (one comment)\n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = delimiters.sub(' ', text) # replace delimiters symbols by space in text\n",
    "    text = ignored_symbols.sub('', text) # delete symbols which are in ignored_symbols from text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "df['comments'] = df['comments'].apply(clean_text)\n",
    "\n",
    "#COMPETITION SET\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to lemmatize the corpus.  This might not help in the short term, but will be useful to play with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  honestli buffalo correct answer rememb peopl s...           hockey\n",
      "1   1  ah ye way could rememb draft thought gon na gr...              nba\n",
      "2   2  http youtub 6xxbbr8isz0t40m49sif didnt find al...  leagueoflegends\n",
      "3   3  wouldnt bad sign wouldnt paid 18m euro right p...           soccer\n",
      "4   4  easi use piss dri techniqu let drop let dri ri...            funny\n",
      "5   5                              joke youiv seen twice            funny\n",
      "6   6  role mi3 one best villian ive seen movi genuin...           movies\n",
      "7   7  akagi still alpha fuck sugawara suffer definit...            anime\n",
      "8   8  think other deton wouldnt proven joker right b...           movies\n",
      "9   9  right disruptor tank pull dp frey pick get poi...        Overwatch\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "#stemmer = LancasterStemmer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_sentence(sen):\n",
    "    \"\"\" lemmatizes every word in space separated sentence sen\"\"\" \n",
    "    token_list = word_tokenize(sen)\n",
    "    lemma_sen = []\n",
    "    for w in token_list:\n",
    "        lemma_sen.append(stemmer.stem(w))\n",
    "    return \" \".join(lemma_sen)\n",
    "\n",
    "df['comments'] = df['comments'].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "# print_plot(1234)\n",
    "print(df.head(10))\n",
    "\n",
    "#COMPETITION SET\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(lambda x: lemmatize_sentence(x))\n",
    "X_kaggle = pd.Series(X_kaggle['comments'], index=X_kaggle.index)\n",
    "# print_plot(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set the train test split here, or we could do some more processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_array = np.unique(df.subreddits.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to get **a list of words in the entire corpus of comments** (that is, `tokens`), and also **a list unique words** (that is `types`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens 1601210\n",
      "number of types 75440\n"
     ]
    }
   ],
   "source": [
    "# import itertools\n",
    "\n",
    "# tokens_list = df['comments'].apply(lambda x: word_tokenize(x)).values\n",
    "\n",
    "# tokens = np.array(list(itertools.chain.from_iterable(tokens_list)))\n",
    "\n",
    "# types, type_counts = np.unique(tokens, return_counts=True)\n",
    "\n",
    "# print(\"number of tokens\",len(tokens))\n",
    "# print(\"number of types\",len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words are more common than others.  We might want to do something with this. But for now it's just useful to have the information.  I'll make an uncommon-word list. We can remove them just like we did the stopwords. We'll do this later, with scikitlearn's CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_words = types[type_counts > 20]\n",
    "# len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsize_vocab(text):\n",
    "#     text = ' '.join(word for word in text.split() if word in common_words) # keep only common words\n",
    "#     return text\n",
    "    \n",
    "# df['comments'] = df['comments'].apply(downsize_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_list = df['comments'].apply(lambda x: word_tokenize(x)).values\n",
    "\n",
    "# tokens = np.array(list(itertools.chain.from_iterable(tokens_list)))\n",
    "\n",
    "# types, type_counts = np.unique(tokens, return_counts=True)\n",
    "\n",
    "# print(\"number of tokens\",len(tokens))\n",
    "# print(\"number of types\",len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is cleand, set the train test split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tfidf = TfidfTransformer()\n",
    "#X_tfidf = tfidf.fit_transform(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df.subreddits, test_size=0.3, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running sklearn classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__estimators': [('logreg', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)), ('mnb', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)), ('cnb', ComplementNB(alpha=1, class_prior=None, fit_prior=True, norm=False))], 'clf__voting': 'hard'}\n",
      "0.5552859862984136\n",
      "accuracy 0.572047619047619\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.24      0.32      0.27      1039\n",
      "GlobalOffensive       0.60      0.67      0.63      1028\n",
      "          Music       0.57      0.69      0.62      1011\n",
      "      Overwatch       0.66      0.71      0.68      1085\n",
      "          anime       0.62      0.64      0.63      1067\n",
      "       baseball       0.68      0.64      0.66      1091\n",
      "         canada       0.42      0.53      0.47      1012\n",
      "     conspiracy       0.40      0.47      0.43      1019\n",
      "         europe       0.55      0.53      0.54      1084\n",
      "          funny       0.23      0.17      0.19      1053\n",
      "  gameofthrones       0.75      0.74      0.75      1050\n",
      "         hockey       0.67      0.62      0.64      1013\n",
      "leagueoflegends       0.74      0.66      0.70      1013\n",
      "         movies       0.61      0.60      0.60      1054\n",
      "            nba       0.72      0.64      0.68      1074\n",
      "            nfl       0.70      0.61      0.65      1042\n",
      "         soccer       0.77      0.62      0.69      1125\n",
      "          trees       0.54      0.55      0.54      1072\n",
      "      worldnews       0.38      0.32      0.34      1048\n",
      "            wow       0.72      0.73      0.72      1020\n",
      "\n",
      "       accuracy                           0.57     21000\n",
      "      macro avg       0.58      0.57      0.57     21000\n",
      "   weighted avg       0.58      0.57      0.57     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_nb = Pipeline([\n",
    "                         ('ct_vect', CountVectorizer(binary=True)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', BernoulliNB(alpha=0.1)),\n",
    "                        ])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "multinomial_nb = Pipeline([\n",
    "                         ('ct_vect', CountVectorizer(binary=False)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB(alpha=0.1)),\n",
    "                        ])\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "complement_nb = Pipeline([\n",
    "                         ('ct_vect', CountVectorizer(binary=False)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', ComplementNB(alpha=1)),\n",
    "                        ])\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', SGDClassifier(loss='modified_huber', penalty='l2',\n",
    "                                       alpha=1e-4, random_state=27,\n",
    "                                       max_iter=5, tol=None)),\n",
    "                ])\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(ngram_range=(1,1))),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', KNeighborsClassifier(n_neighbors=300, weights = 'distance')),\n",
    "                ])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()), # works well with more features (all 75440 ...)\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', LogisticRegression(penalty='l2',multi_class='ovr',solver='saga')),\n",
    "                ])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randfrst = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(max_features=5000)),\n",
    "                 ('feature_selection', SelectFromModel(LinearSVC(C=0.01, penalty=\"l1\", dual=False))), # optional\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', RandomForestClassifier(n_estimators=10)), # more estimators might be good, but slow\n",
    "                ])\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer(max_features=5000)),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MLPClassifier(alpha=1, max_iter=100)),\n",
    "                ])\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "passaggr = Pipeline([\n",
    "                 ('ct_vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', PassiveAggressiveClassifier(fit_intercept=True,C=0.1)),\n",
    "                ])\n",
    "\n",
    "# now for comparing and majority voting:\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf1 = LogisticRegression(penalty='l2',multi_class='ovr',solver='saga')\n",
    "clf2 = MultinomialNB(alpha=0.1)\n",
    "clf3 = ComplementNB(alpha=1)\n",
    "clf4 = PassiveAggressiveClassifier(fit_intercept=True,C=0.1) # won't work with voting='soft'\n",
    "clf5 = SGDClassifier(loss='modified_huber', penalty='l2',alpha=1e-4, random_state=27, max_iter=5, tol=None)\n",
    "clf6 = KNeighborsClassifier(n_neighbors=300, weights = 'distance')\n",
    "\n",
    "voting_clf = Pipeline([\n",
    "    ('ct_vect', CountVectorizer(binary=False)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf' , VotingClassifier(estimators=\n",
    "                              [('logreg', clf1), ('mnb', clf2), ('cnb', clf3)],\n",
    "                              voting='soft',n_jobs=-1))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paramsearch(modelpipeline):\n",
    "    '''\n",
    "    modelpipeline: sklearn.pipeline.Pipeline object \n",
    "    does gridsearch on the given pipeline on test split and prints classification report\n",
    "    '''\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    parameters = {\n",
    "        #passagr\n",
    "#         'clf__C':[0.5,0.1,0.01]\n",
    "        #voting\n",
    "        'clf__voting':['hard','soft'],\n",
    "        'clf__estimators':[\n",
    "            [('logreg', clf1), ('mnb', clf2), ('cnb', clf3)],\n",
    "            [('logreg', clf1), ('mnb', clf2), ('cnb', clf3),('sgd', clf5)],\n",
    "        ]\n",
    "        #logreg\n",
    "#         'ct_vect__ngram_range': [(1,1)],\n",
    "#         'ct_vect__max_features': [1000,5000,74000],\n",
    "#         'clf__multi_class':['ovr','multinomial'],\n",
    "#         'clf__solver':['sag','saga'],\n",
    "        \n",
    "#       #knn\n",
    "#         'ct_vect__ngram_range': [(1,1)],\n",
    "#         'ct_vect__max_features': [5000,74000],\n",
    "#         'clf__n_neighbors':[3,30],\n",
    "        \n",
    "        #bernoulli_nb\n",
    "#         'ct_vect__ngram_range': [(1,1)],\n",
    "#         'ct_vect__max_features': [10000],\n",
    "#         'clf__alpha': [1,1e-2,1e-3],\n",
    "        \n",
    "        #sgd\n",
    "#         'ct_vect__ngram_range': [(1,1),(1,2)],\n",
    "#         'ct_vect__max_features': [5000],\n",
    "#         'clf__alpha': [1e-3, 1e-5],\n",
    "    }\n",
    "    # run gridsearch for parameters with 3-fold cross validation\n",
    "    gridsearch = GridSearchCV(modelpipeline, parameters, cv=3, iid=False, n_jobs=-1)\n",
    "    gridsearch = gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "#     print(gridsearch)\n",
    "    print(gridsearch.best_params_)\n",
    "    print(gridsearch.best_score_)\n",
    "    print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, target_names=values_array))\n",
    "\n",
    "    return gridsearch\n",
    "\n",
    "def runmodel(modelpipeline):\n",
    "    '''\n",
    "    modelpipeline: sklearn.pipeline.Pipeline object \n",
    "    runs the given pipeline on test split and prints classification report\n",
    "    '''\n",
    "    modelpipeline.fit(X_train, y_train)\n",
    "    y_pred = modelpipeline.predict(X_test)\n",
    "    print(modelpipeline)\n",
    "    print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred,target_names=values_array))\n",
    "\n",
    "gridsearch = paramsearch(voting_clf)\n",
    "\n",
    "# runmodel(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29995</td>\n",
       "      <td>29995</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29996</td>\n",
       "      <td>29996</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29997</td>\n",
       "      <td>29997</td>\n",
       "      <td>GlobalOffensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29998</td>\n",
       "      <td>29998</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29999</td>\n",
       "      <td>29999</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id         Category\n",
       "0          0         baseball\n",
       "1          1           europe\n",
       "2          2            anime\n",
       "3          3        worldnews\n",
       "4          4           movies\n",
       "...      ...              ...\n",
       "29995  29995           movies\n",
       "29996  29996        AskReddit\n",
       "29997  29997  GlobalOffensive\n",
       "29998  29998    gameofthrones\n",
       "29999  29999              wow\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAKE PREDICTIONS FOR THE HELDOUT COMPETITION SET\n",
    "y_kaggle = gridsearch.predict(X_kaggle)\n",
    "y_kaggle = pd.Series(y_kaggle, index=X_kaggle.index)\n",
    "y_kaggle = pd.DataFrame(y_kaggle)\n",
    "y_kaggle['Id_'] = y_kaggle.index\n",
    "y_kaggle.insert(loc = 0, column = \"Id\", value = y_kaggle['Id_'])\n",
    "y_kaggle = y_kaggle.drop(columns=['Id_'])\n",
    "y_kaggle = y_kaggle.rename(columns={0: \"Category\"})\n",
    "y_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kaggle.to_csv(\"predictions.csv\", index=False, sep = ',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes(observations, y, num_features,smoothing):\n",
    "\n",
    "    #Initialize marginal probability for each class\n",
    "    count_class = np.array(20*[[0]])\n",
    "    marg_prob = np.array(20*[[1]]) #Laplace smoothing, starting counts with 1\n",
    "\n",
    "    #Initialize matrix of probabilities of observed features given k\n",
    "    cond_prob_matrix = np.ones((20,num_features)) * smoothing\n",
    "\n",
    "    \n",
    "    #compute marginal probability of each class\n",
    "    total_comments = y.shape[0]\n",
    "    for j in range(total_comments):\n",
    "        count_class[y[j]] += 1\n",
    "    \n",
    "    #Marginal probability for each class\n",
    "    marg_prob = np.true_divide(count_class, total_comments)\n",
    "\n",
    "    \n",
    "    observ = observations.nonzero()\n",
    "    for i in range(observations.shape[0]):\n",
    "        feature_no = observ[1][i]\n",
    "        comment_no = observ[0][i]\n",
    "       \n",
    "        comment_class = y[comment_no]\n",
    "        cond_prob_matrix[comment_class][feature_no] += 1\n",
    "\n",
    "    #divide each row of cond_prob_matrix by the count of comments per class\n",
    "    for i in range(20):\n",
    "        cond_prob_matrix[i] = np.true_divide(cond_prob_matrix[i], count_class[i])\n",
    "\n",
    "\n",
    "    cond_prob_matrix = cond_prob_matrix.transpose()\n",
    "    marg_prob = np.log(marg_prob)\n",
    "\n",
    "    return marg_prob, cond_prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "        \"anime\": 1,\n",
    "        \"AskReddit\": 2,\n",
    "        \"baseball\": 3,\n",
    "        \"canada\": 4, \n",
    "        \"conspiracy\": 5, \n",
    "        \"europe\": 6, \n",
    "        \"funny\": 7, \n",
    "        \"gameofthrones\": 8, \n",
    "        \"GlobalOffensive\": 9,\n",
    "        \"hockey\" :10, \n",
    "        \"leagueoflegends\": 11, \n",
    "        \"movies\": 12, \n",
    "        \"Music\": 13, \n",
    "        \"nba\":14, \n",
    "        \"nfl\":15, \n",
    "        \"Overwatch\":16, \n",
    "        \"soccer\":17, \n",
    "        \"trees\":18, \n",
    "        \"worldnews\":19, \n",
    "        \"wow\":0\n",
    "    }\n",
    "\n",
    "y_traindf = pd.DataFrame(y_train)\n",
    "y_traindf['subreddits']= y_traindf['subreddits'].map(classes)\n",
    "y_train_array = np.array(y_traindf['subreddits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5000,binary=True)\n",
    "X_train_tf = cv.fit_transform(X_train)\n",
    "X_test_tf = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.20167088508605957 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# from naive_bayes import fit_naive_bayes\n",
    "prior, conditional = fit_naive_bayes(X_train_tf, y_train_array, X_train_tf.shape[1],0.1)\n",
    "\n",
    "print(\"Took %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def predict_naive_bayes(id_list, observations, marg_prob, cond_prob_matrix):\n",
    "\n",
    "    #log of inverse conditional probability matrix\n",
    "    inv_cond_prob_matrix = np.ones((cond_prob_matrix.shape[0], cond_prob_matrix.shape[1]))\n",
    "    inv_cond_prob_matrix = inv_cond_prob_matrix - cond_prob_matrix\n",
    "    inv_cond_prob_matrix = sparse.csr_matrix(np.log(inv_cond_prob_matrix))\n",
    "\n",
    "    #log of conditional probability matrix\n",
    "    cond_prob_matrix = sparse.csr_matrix(np.log(cond_prob_matrix))\n",
    "    \n",
    "    # 0s become 1s, 1s become 0s\n",
    "    sparse_ones = sparse.csr_matrix(np.ones((observations.shape[0], observations.shape[1])))\n",
    "    complement_obs = sparse_ones - observations\n",
    "\n",
    "    prob_per_class = np.dot(observations,cond_prob_matrix) + np.dot(complement_obs,inv_cond_prob_matrix)\n",
    "\n",
    "    y = []\n",
    "    for i in range(observations.shape[0]):\n",
    "        prob_per_class[i] += marg_prob.transpose()\n",
    "        y.append(np.argmax(prob_per_class[i]))\n",
    "\n",
    "    id_list = np.array(id_list).transpose()\n",
    "\n",
    "    matrix = np.stack((id_list, y)).transpose()\n",
    "    df_pred = pd.DataFrame(matrix)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_naive_bayes(ID_list, X_test_tf, prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>26505</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16099</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35596</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62735</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>67323</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20995</td>\n",
       "      <td>23873</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20996</td>\n",
       "      <td>15156</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20997</td>\n",
       "      <td>1645</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20998</td>\n",
       "      <td>4919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20999</td>\n",
       "      <td>50805</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0   1\n",
       "0      26505   2\n",
       "1      16099  19\n",
       "2      35596   2\n",
       "3      62735   8\n",
       "4      67323   6\n",
       "...      ...  ..\n",
       "20995  23873   9\n",
       "20996  15156  18\n",
       "20997   1645  15\n",
       "20998   4919   4\n",
       "20999  50805   4\n",
       "\n",
       "[21000 rows x 2 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testdf = pd.DataFrame(y_test)\n",
    "y_testdf['subreddits']= y_testdf['subreddits'].map(classes)\n",
    "y_test_array = np.array(y_testdf['subreddits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1        19\n",
       "2         2\n",
       "3         8\n",
       "4         6\n",
       "         ..\n",
       "20995     9\n",
       "20996    18\n",
       "20997    15\n",
       "20998     4\n",
       "20999     4\n",
       "Name: 1, Length: 21000, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df_pred, df_true_y):\n",
    "\n",
    "    pred = np.array(df_pred[1])\n",
    "    true_y = np.array(df_true_y['subreddits'])\n",
    "\n",
    "    count = 0\n",
    "    total = len(true_y)\n",
    "    for i in range(total):\n",
    "        if pred[i] == true_y[i]:\n",
    "            count +=1\n",
    "            \n",
    "    return float(count)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.2924285714285714\n"
     ]
    }
   ],
   "source": [
    "accuracy(predictions, y_testdf)\n",
    "print(classification_report(y_testdf['subreddits'], predictions[1],target_names=values_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnotherBernoulliNB(object):\n",
    "    ''' based on https://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html '''\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [np.log(len(i) / count_sample) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        smoothing = 2 * self.alpha\n",
    "        n_doc = np.array([len(i) + smoothing for i in separated])\n",
    "        self.feature_prob_ = count / n_doc[np.newaxis].T\n",
    "        return self\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        return [(np.log(self.feature_prob_) * x + \\\n",
    "                 np.log(1 - self.feature_prob_) * np.abs(x - 1)\n",
    "                ).sum(axis=1) + self.class_log_prior_ for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = BernoulliNB(alpha=0.1)\n",
    "nb.fit(X_train_tf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpredictions = nb.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>26505</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16099</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35596</td>\n",
       "      <td>AskReddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62735</td>\n",
       "      <td>gameofthrones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67323</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23873</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15156</td>\n",
       "      <td>trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4919</td>\n",
       "      <td>trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50805</td>\n",
       "      <td>nfl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddits\n",
       "26505          funny\n",
       "16099      AskReddit\n",
       "35596      AskReddit\n",
       "62735  gameofthrones\n",
       "67323       baseball\n",
       "...              ...\n",
       "23873          funny\n",
       "15156          trees\n",
       "1645          soccer\n",
       "4919           trees\n",
       "50805            nfl\n",
       "\n",
       "[21000 rows x 1 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame(y_test)\n",
    "prediction_df['subreddits'] = pd.DataFrame(newpredictions)[0].values\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.45871428571428574\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.40      0.45      0.42      1020\n",
      "GlobalOffensive       0.29      0.42      0.34      1067\n",
      "          Music       0.12      0.12      0.12      1039\n",
      "      Overwatch       0.49      0.15      0.23      1091\n",
      "          anime       0.15      0.58      0.24      1012\n",
      "       baseball       0.20      0.22      0.21      1019\n",
      "         canada       0.28      0.29      0.28      1084\n",
      "     conspiracy       0.17      0.05      0.08      1053\n",
      "         europe       0.42      0.54      0.47      1050\n",
      "          funny       0.40      0.30      0.34      1028\n",
      "  gameofthrones       0.35      0.24      0.28      1013\n",
      "         hockey       0.37      0.31      0.33      1013\n",
      "leagueoflegends       0.29      0.24      0.26      1054\n",
      "         movies       0.31      0.52      0.39      1011\n",
      "            nba       0.49      0.22      0.30      1074\n",
      "            nfl       0.49      0.18      0.26      1042\n",
      "         soccer       0.45      0.44      0.44      1085\n",
      "          trees       0.41      0.20      0.27      1125\n",
      "      worldnews       0.34      0.28      0.31      1072\n",
      "            wow       0.16      0.15      0.15      1048\n",
      "\n",
      "       accuracy                           0.29     21000\n",
      "      macro avg       0.33      0.29      0.29     21000\n",
      "   weighted avg       0.33      0.29      0.29     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_test, prediction_df))\n",
    "print(classification_report(y_testdf['subreddits'], predictions[1],target_names=values_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
