{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comp551miniproj2 - reddit text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "5   5  The joke is on YOU!\\n\\nI've only seen it twice...            funny\n",
      "6   6  His role in MI3 is one of the best villians I'...           movies\n",
      "7   7  Akagi is still Alpha as fuck and Sugawara is s...            anime\n",
      "8   8  I think that they had each other's detonator. ...           movies\n",
      "9   9  Right! He was a disruptor tank! Pull the dps o...        Overwatch\n",
      "2968210\n",
      "   id                                           comments\n",
      "0   0  Trout and Bryant have both led the league in s...\n",
      "1   1  &gt; Just like Estonians have good reasons to ...\n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4   4         You have to take it out of the bag, Morty!\n",
      "5   5  Don't forget the obnoxious \"*memes*\" in every ...\n",
      "6   6  I say encourage local team support. Half the f...\n",
      "7   7  Favorite type of pasta? (not dish, pasta shape...\n",
      "8   8  Spinal meningitis- Ween.\\n\\nOn mobile, so no l...\n",
      "9   9  So what about Scandinavians, Caucasians, Asian...\n",
      "1257851\n"
     ]
    }
   ],
   "source": [
    "#VARIOUS IMPORTS AND CSV READ INTO DATAFRAME\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import sparse\n",
    "import time\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('reddit-comment-classification-comp-551/reddit_train.csv')\n",
    "df = df[pd.notnull(df['comments'])]\n",
    "print(df.head(10))\n",
    "print(df['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "X_kaggle = pd.read_csv('reddit-comment-classification-comp-551/reddit_test.csv')\n",
    "X_kaggle = X_kaggle[pd.notnull(X_kaggle['comments'])]\n",
    "print(X_kaggle.head(10))\n",
    "print(X_kaggle['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are balanced, but the text needs cleaning. Here's some cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISCELLANEOUS PROCESSING\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "delimiters = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "ignored_symbols = re.compile('[^0-9a-z #+_]')\n",
    "# nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string (one comment)\n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding # our pipeline does slightly better without this.\n",
    "    text = text.lower() # lowercase text\n",
    "    text = delimiters.sub(' ', text) # replace delimiters symbols by space in text\n",
    "    text = ignored_symbols.sub('', text) # delete symbols which are in ignored_symbols from text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "df['comments'] = df['comments'].apply(clean_text)\n",
    "\n",
    "#COMPETITION SET\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to lemmatize the corpus.  This might not help in the short term, but will be useful to play with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  honestli buffalo correct answer rememb peopl s...           hockey\n",
      "1   1  ah ye way could rememb draft thought gon na gr...              nba\n",
      "2   2  http youtub 6xxbbr8isz0t40m49sif didnt find al...  leagueoflegends\n",
      "3   3  wouldnt bad sign wouldnt paid 18m euro right p...           soccer\n",
      "4   4  easi use piss dri techniqu let drop let dri ri...            funny\n",
      "5   5                              joke youiv seen twice            funny\n",
      "6   6  role mi3 one best villian ive seen movi genuin...           movies\n",
      "7   7  akagi still alpha fuck sugawara suffer definit...            anime\n",
      "8   8  think other deton wouldnt proven joker right b...           movies\n",
      "9   9  right disruptor tank pull dp frey pick get poi...        Overwatch\n"
     ]
    }
   ],
   "source": [
    "#STEMMERIZE THE WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer\n",
    "#stemmer = LancasterStemmer()\n",
    "stemmer = PorterStemmer()\n",
    "stemmer1 = LancasterStemmer()\n",
    "\n",
    "def stem_sentence(sen):\n",
    "    \"\"\" stems every word in space separated sentence sen \"\"\" \n",
    "    token_list = word_tokenize(sen)\n",
    "    stem_sen = []\n",
    "    for w in token_list:\n",
    "#         w= stemmer1.stem(w)\n",
    "        stem_sen.append(stemmer.stem(w))\n",
    "    return \" \".join(stem_sen)\n",
    "\n",
    "# Download wordnet (could take a little while!), to do lemmatization\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# def lemm_sentence(sen):\n",
    "#     \"\"\" lemmatizes every word in space separated sentence sen \"\"\" \n",
    "#     token_list = word_tokenize(sen)\n",
    "#     lemma_sen = []\n",
    "#     for w in token_list:\n",
    "#         lemma_sen.append(lemmatizer.lemmatize(w))\n",
    "#     return \" \".join(lemma_sen)\n",
    "\n",
    "\n",
    "df['comments'] = df['comments'].apply(lambda x: stem_sentence(x))\n",
    "\n",
    "print(df.head(10))\n",
    "\n",
    "#COMPETITION SET\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(lambda x: stem_sentence(x))\n",
    "X_kaggle = pd.Series(X_kaggle['comments'], index=X_kaggle.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET DISTINCT CLASS NUMBERS FOR ACCURACY LATER\n",
    "values_array = np.unique(df.subreddits.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT WHOLE SET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIT NAIVE BAYES IMPLEMENTATION\n",
    "def fit_naive_bayes(observations, y, num_features,smoothing):\n",
    "\n",
    "    #Initialize marginal probability for each class\n",
    "    count_class = np.array(20*[[0]])\n",
    "    marg_prob = np.array(20*[[0]]) #Laplace smoothing, starting counts with 1\n",
    "\n",
    "    #Initialize matrix of probabilities of observed features given k\n",
    "    cond_prob_matrix = np.ones((20,num_features)) * smoothing\n",
    "\n",
    "    \n",
    "    #compute marginal probability of each class\n",
    "    total_comments = y.shape[0]\n",
    "    for j in range(total_comments):\n",
    "        count_class[y[j]] += 1\n",
    "    \n",
    "    #Marginal probability for each class\n",
    "    marg_prob = np.true_divide(count_class, total_comments)\n",
    "    \n",
    "    observ = observations.nonzero()\n",
    "    j = 0 #counter of comments\n",
    "    prev_comment_no = observ[0][0] #counter to see if next comment\n",
    "    for i in range(observations.shape[0]):\n",
    "        \n",
    "        feature_no = observ[1][i]\n",
    "        comment_no = observ[0][i]\n",
    "        \n",
    "        if prev_comment_no != comment_no:\n",
    "            j += comment_no - prev_comment_no\n",
    "            prev_comment_no = comment_no\n",
    "            \n",
    "        comment_class = y[j]\n",
    "        cond_prob_matrix[comment_class][feature_no] += 1\n",
    "\n",
    "    #divide each row of cond_prob_matrix by the count of comments per class\n",
    "    for i in range(20):\n",
    "        cond_prob_matrix[i] = np.true_divide(cond_prob_matrix[i], count_class[i])\n",
    "\n",
    "    cond_prob_matrix = cond_prob_matrix.transpose()\n",
    "    marg_prob = np.log(marg_prob)\n",
    "\n",
    "    #marg_prob is a vector of 20, cond_prob_matrix a matrix #features rows by 20\n",
    "    return marg_prob, cond_prob_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT CLASSES TO INT\n",
    "classes = {\n",
    "        \"anime\": 1,\n",
    "        \"AskReddit\": 2,\n",
    "        \"baseball\": 3,\n",
    "        \"canada\": 4, \n",
    "        \"conspiracy\": 5, \n",
    "        \"europe\": 6, \n",
    "        \"funny\": 7, \n",
    "        \"gameofthrones\": 8, \n",
    "        \"GlobalOffensive\": 9,\n",
    "        \"hockey\" :10, \n",
    "        \"leagueoflegends\": 11, \n",
    "        \"movies\": 12, \n",
    "        \"Music\": 13, \n",
    "        \"nba\":14, \n",
    "        \"nfl\":15, \n",
    "        \"Overwatch\":16, \n",
    "        \"soccer\":17, \n",
    "        \"trees\":18, \n",
    "        \"worldnews\":19, \n",
    "        \"wow\":0\n",
    "    }\n",
    "\n",
    "y_traindf = pd.DataFrame(y_train)\n",
    "y_traindf['subreddits']= y_traindf['subreddits'].map(classes)\n",
    "y_train_array = np.array(y_traindf['subreddits'])\n",
    "\n",
    "y_testdf = pd.DataFrame(y_test)\n",
    "y_testdf['subreddits']= y_testdf['subreddits'].map(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS DE WORDS AND TURN OBSERVATIONS TO BINARY\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000,binary=True)\n",
    "X_train_tf = cv.fit_transform(X_train)\n",
    "X_test_tf = cv.transform(X_test)\n",
    "ID_list = X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.2766687870025635 seconds.\n"
     ]
    }
   ],
   "source": [
    "#FIT OUR NB MODEL\n",
    "start_time = time.time()\n",
    "prior, conditional = fit_naive_bayes(X_train_tf, y_train_array, X_train_tf.shape[1],0.01)\n",
    "print(\"Took %s seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT FUNCTION\n",
    "def predict_naive_bayes(id_list, observations, marg_prob, cond_prob_matrix):\n",
    "\n",
    "    #log of inverse conditional probability matrix\n",
    "    inv_cond_prob_matrix = np.ones((cond_prob_matrix.shape[0], cond_prob_matrix.shape[1]), dtype=int)\n",
    "    inv_cond_prob_matrix = inv_cond_prob_matrix - cond_prob_matrix\n",
    "    inv_cond_prob_matrix = sparse.csr_matrix(np.log(inv_cond_prob_matrix))\n",
    "\n",
    "    #log of conditional probability matrix\n",
    "    cond_prob_matrix = sparse.csr_matrix(np.log(cond_prob_matrix))\n",
    "    \n",
    "    # 0s become 1s, 1s become 0s\n",
    "    sparse_ones = sparse.csr_matrix(np.ones((observations.shape[0], observations.shape[1])), dtype=int)\n",
    "    complement_obs = sparse_ones - observations\n",
    "    \n",
    "    prob_per_class = observations.dot(cond_prob_matrix) + complement_obs.dot(inv_cond_prob_matrix)\n",
    "    \n",
    "    y = []\n",
    "    for i in range(observations.shape[0]):\n",
    "        prob_per_class[i] += marg_prob.transpose()\n",
    "        y.append(np.argmax(prob_per_class[i]))\n",
    "        \n",
    "    id_list = np.array(id_list).transpose()\n",
    "\n",
    "    matrix = np.stack((id_list, y)).transpose()\n",
    "    df_pred = pd.DataFrame(matrix)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_naive_bayes(ID_list, X_test_tf, prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.32      0.53      0.40      1061\n",
      "GlobalOffensive       0.23      0.33      0.27      1010\n",
      "          Music       0.14      0.25      0.17      1052\n",
      "      Overwatch       0.42      0.25      0.31      1097\n",
      "          anime       0.26      0.24      0.25      1038\n",
      "       baseball       0.26      0.23      0.25      1081\n",
      "         canada       0.23      0.34      0.27      1053\n",
      "     conspiracy       0.13      0.03      0.05      1043\n",
      "         europe       0.57      0.39      0.46      1080\n",
      "          funny       0.40      0.26      0.31      1005\n",
      "  gameofthrones       0.37      0.24      0.29      1034\n",
      "         hockey       0.40      0.31      0.35      1021\n",
      "leagueoflegends       0.31      0.24      0.27      1045\n",
      "         movies       0.26      0.52      0.35      1064\n",
      "            nba       0.45      0.30      0.36      1095\n",
      "            nfl       0.40      0.30      0.35      1034\n",
      "         soccer       0.35      0.55      0.43      1065\n",
      "          trees       0.47      0.18      0.26      1001\n",
      "      worldnews       0.30      0.23      0.26      1031\n",
      "            wow       0.19      0.24      0.21      1090\n",
      "\n",
      "       accuracy                           0.30     21000\n",
      "      macro avg       0.32      0.30      0.29     21000\n",
      "   weighted avg       0.32      0.30      0.29     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_testdf['subreddits'], predictions[1],target_names=values_array))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
