{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments       subreddits\n",
      "0   0  Honestly, Buffalo is the correct answer. I rem...           hockey\n",
      "1   1  Ah yes way could have been :( remember when he...              nba\n",
      "2   2  https://youtu.be/6xxbBR8iSZ0?t=40m49s\\n\\nIf yo...  leagueoflegends\n",
      "3   3  He wouldn't have been a bad signing if we woul...           soccer\n",
      "4   4  Easy. You use the piss and dry technique. Let ...            funny\n",
      "5   5  The joke is on YOU!\\n\\nI've only seen it twice...            funny\n",
      "6   6  His role in MI3 is one of the best villians I'...           movies\n",
      "7   7  Akagi is still Alpha as fuck and Sugawara is s...            anime\n",
      "8   8  I think that they had each other's detonator. ...           movies\n",
      "9   9  Right! He was a disruptor tank! Pull the dps o...        Overwatch\n",
      "2968210\n",
      "   id                                           comments\n",
      "0   0  Trout and Bryant have both led the league in s...\n",
      "1   1  &gt; Just like Estonians have good reasons to ...\n",
      "2   2  Will Sol_Primeval sotp being oblivious?\\n\\nfin...\n",
      "3   3  Moving Ostwald borders back to the pre 1967 bo...\n",
      "4   4         You have to take it out of the bag, Morty!\n",
      "5   5  Don't forget the obnoxious \"*memes*\" in every ...\n",
      "6   6  I say encourage local team support. Half the f...\n",
      "7   7  Favorite type of pasta? (not dish, pasta shape...\n",
      "8   8  Spinal meningitis- Ween.\\n\\nOn mobile, so no l...\n",
      "9   9  So what about Scandinavians, Caucasians, Asian...\n",
      "1257851\n"
     ]
    }
   ],
   "source": [
    "#VARIOUS IMPORTS AND CSV READ INTO DATAFRAME\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from scipy import sparse\n",
    "import time\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('reddit-comment-classification-comp-551/reddit_train.csv')\n",
    "df = df[pd.notnull(df['comments'])]\n",
    "print(df.head(10))\n",
    "print(df['comments'].apply(lambda x: len(x.split(' '))).sum())\n",
    "\n",
    "X_kaggle = pd.read_csv('reddit-comment-classification-comp-551/reddit_test.csv')\n",
    "X_kaggle = X_kaggle[pd.notnull(X_kaggle['comments'])]\n",
    "print(X_kaggle.head(10))\n",
    "print(X_kaggle['comments'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISCELLANEOUS PROCESSING\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "delimiters = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "ignored_symbols = re.compile('[^0-9a-z #+_]')\n",
    "# nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string (one comment)\n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding # our pipeline does slightly better without this.\n",
    "    text = text.lower() # lowercase text\n",
    "    text = delimiters.sub(' ', text) # replace delimiters symbols by space in text\n",
    "    text = ignored_symbols.sub('', text) # delete symbols which are in ignored_symbols from text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "df['comments'] = df['comments'].apply(clean_text)\n",
    "\n",
    "#COMPETITION SET\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           comments subreddits\n",
      "0   0  honestli buffalo correct answer rememb peopl s...     hockey\n"
     ]
    }
   ],
   "source": [
    "#STEMMERIZE THE WORdS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_sentence(sen):\n",
    "    \"\"\" stems every word in space separated sentence sen \"\"\" \n",
    "    token_list = word_tokenize(sen)\n",
    "    stem_sen = []\n",
    "    for w in token_list:\n",
    "        stem_sen.append(stemmer.stem(w))\n",
    "    return \" \".join(stem_sen)\n",
    "\n",
    "\n",
    "# choose lemm_sentence)() for lemmatization, or stem_sentence() for stemming.\n",
    "df['comments'] = df['comments'].apply(lambda x: stem_sentence(x))\n",
    "\n",
    "# print_plot(1234)\n",
    "print(df.head(1))\n",
    "\n",
    "#COMPETITION SET\n",
    "# choose lemm_sentence)() for lemmatization, or stem_sentence() for stemming.\n",
    "X_kaggle['comments'] = X_kaggle['comments'].apply(lambda x: stem_sentence(x))\n",
    "X_kaggle = pd.Series(X_kaggle['comments'], index=X_kaggle.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.comments, df.subreddits, test_size=0.3, random_state = 27)\n",
    "values_array = np.unique(df.subreddits.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIT NAIVE BAYES IMPLEMENTATION\n",
    "def fit_naive_bayes(observations, y, num_features,smoothing):\n",
    "\n",
    "    #Initialize marginal probability for each class\n",
    "    count_class = np.array(20*[[0]])\n",
    "    marg_prob = np.array(20*[[0]]) #Laplace smoothing, starting counts with 1\n",
    "\n",
    "    #Initialize matrix of probabilities of observed features given k\n",
    "    cond_prob_matrix = np.ones((20,num_features)) * smoothing\n",
    "\n",
    "    \n",
    "    #compute marginal probability of each class\n",
    "    total_comments = y.shape[0]\n",
    "    for j in range(total_comments):\n",
    "        count_class[y[j]] += 1\n",
    "    \n",
    "    #Marginal probability for each class\n",
    "    marg_prob = np.true_divide(count_class, total_comments)\n",
    "    \n",
    "    observ = observations.nonzero()\n",
    "    j = 0 #counter of comments\n",
    "    prev_comment_no = observ[0][0] #counter to see if next comment\n",
    "    for i in range(observations.shape[0]):\n",
    "        \n",
    "        feature_no = observ[1][i]\n",
    "        comment_no = observ[0][i]\n",
    "        \n",
    "        if prev_comment_no != comment_no:\n",
    "            j += comment_no - prev_comment_no\n",
    "            prev_comment_no = comment_no\n",
    "            \n",
    "        comment_class = y[j]\n",
    "        cond_prob_matrix[comment_class][feature_no] += 1\n",
    "\n",
    "    #divide each row of cond_prob_matrix by the count of comments per class\n",
    "    for i in range(20):\n",
    "        cond_prob_matrix[i] = np.true_divide(cond_prob_matrix[i], count_class[i])\n",
    "\n",
    "    cond_prob_matrix = cond_prob_matrix.transpose()\n",
    "    marg_prob = np.log(marg_prob)\n",
    "\n",
    "    #marg_prob is a vector of 20, cond_prob_matrix a matrix #features rows by 20\n",
    "    return marg_prob, cond_prob_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT CLASSES TO INT\n",
    "classes = {\n",
    "        \"anime\": 1,\n",
    "        \"AskReddit\": 2,\n",
    "        \"baseball\": 3,\n",
    "        \"canada\": 4, \n",
    "        \"conspiracy\": 5, \n",
    "        \"europe\": 6, \n",
    "        \"funny\": 7, \n",
    "        \"gameofthrones\": 8, \n",
    "        \"GlobalOffensive\": 9,\n",
    "        \"hockey\" :10, \n",
    "        \"leagueoflegends\": 11, \n",
    "        \"movies\": 12, \n",
    "        \"Music\": 13, \n",
    "        \"nba\":14, \n",
    "        \"nfl\":15, \n",
    "        \"Overwatch\":16, \n",
    "        \"soccer\":17, \n",
    "        \"trees\":18, \n",
    "        \"worldnews\":19, \n",
    "        \"wow\":0\n",
    "    }\n",
    "\n",
    "y_traindf = pd.DataFrame(y_train)\n",
    "y_traindf['subreddits']= y_traindf['subreddits'].map(classes)\n",
    "\n",
    "y_testdf = pd.DataFrame(y_test)\n",
    "y_testdf['subreddits']= y_testdf['subreddits'].map(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS DE WORDS AND TURN OBSERVATIONS TO BINARY\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000,binary=True)\n",
    "X_train_tf = cv.fit_transform(X_train)\n",
    "X_test_tf = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.3759932518005371 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# from naive_bayes import fit_naive_bayes\n",
    "prior, conditional = fit_naive_bayes(X_train_tf, y_train_array, X_train_tf.shape[1],0.01)\n",
    "\n",
    "print(\"Took %s seconds.\" % (time.time() - start_time))\n",
    "ID_list = X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_list = X_test.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT FUNCTION\n",
    "def predict_naive_bayes(id_list, observations, marg_prob, cond_prob_matrix):\n",
    "\n",
    "    #log of inverse conditional probability matrix\n",
    "    inv_cond_prob_matrix = np.ones((cond_prob_matrix.shape[0], cond_prob_matrix.shape[1]), dtype=int)\n",
    "    inv_cond_prob_matrix = inv_cond_prob_matrix - cond_prob_matrix\n",
    "    inv_cond_prob_matrix = sparse.csr_matrix(np.log(inv_cond_prob_matrix))\n",
    "\n",
    "    #log of conditional probability matrix\n",
    "    cond_prob_matrix = sparse.csr_matrix(np.log(cond_prob_matrix))\n",
    "    \n",
    "    # 0s become 1s, 1s become 0s\n",
    "    sparse_ones = sparse.csr_matrix(np.ones((observations.shape[0], observations.shape[1])), dtype=int)\n",
    "    complement_obs = sparse_ones - observations\n",
    "    \n",
    "    prob_per_class = observations.dot(cond_prob_matrix) + complement_obs.dot(inv_cond_prob_matrix)\n",
    "    \n",
    "    y = []\n",
    "    for i in range(observations.shape[0]):\n",
    "        prob_per_class[i] += marg_prob.transpose()\n",
    "        y.append(np.argmax(prob_per_class[i]))\n",
    "        \n",
    "    id_list = np.array(id_list).transpose()\n",
    "\n",
    "    matrix = np.stack((id_list, y)).transpose()\n",
    "    df_pred = pd.DataFrame(matrix)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTIONS\n",
    "predictions = predict_naive_bayes(ID_list, X_test_tf, prior, conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      AskReddit       0.20      0.59      0.30      1063\n",
      "GlobalOffensive       0.28      0.20      0.23      1056\n",
      "          Music       0.12      0.16      0.14      1003\n",
      "      Overwatch       0.43      0.05      0.08      1060\n",
      "          anime       0.16      0.39      0.22      1030\n",
      "       baseball       0.17      0.17      0.17      1047\n",
      "         canada       0.24      0.28      0.26      1039\n",
      "     conspiracy       0.20      0.02      0.03      1084\n",
      "         europe       0.34      0.50      0.40      1083\n",
      "          funny       0.67      0.07      0.13      1030\n",
      "  gameofthrones       0.29      0.22      0.25      1059\n",
      "         hockey       0.49      0.08      0.14      1043\n",
      "leagueoflegends       0.34      0.08      0.13      1051\n",
      "         movies       0.27      0.47      0.34      1050\n",
      "            nba       0.42      0.18      0.25      1025\n",
      "            nfl       0.29      0.34      0.31      1055\n",
      "         soccer       0.22      0.49      0.30      1049\n",
      "          trees       0.39      0.14      0.20      1053\n",
      "      worldnews       0.42      0.06      0.11      1074\n",
      "            wow       0.15      0.22      0.18      1046\n",
      "\n",
      "       accuracy                           0.24     21000\n",
      "      macro avg       0.30      0.23      0.21     21000\n",
      "   weighted avg       0.30      0.24      0.21     21000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values_array = np.unique(df.subreddits.values)\n",
    "print(classification_report(y_testdf['subreddits'], predictions[1],target_names=values_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
